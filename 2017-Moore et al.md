# “Excellence R Us”: university research and the fetishisation of excellence
#readingnote 


## Metadata

|   Variable     |  |
|:--------------|:-----------|
| **Author**			| Moore, Neylon, Eve, O’Donnell, Pattinson     | 
| **Year**				| 	2017		 | 
| **Title**				| 	'“Excellence R Us”: university research and the fetishisation of excellence'		 | 
| **Kind**				| Article| 
| **Journal/Publisher**				| 	*Palgrave Communications*		 | 
| **Date read**				| 	11 Jun 2021	 | 
| **Weblink**				| 			 | 

**Take-home message**:

---

## Highlights and notes


>This article examines the utility of “excellence” as a means for organizing, funding, and rewarding science and scholarship. It argues that academic research and teaching is not well served by this rhetoric. Nor, we argue, is it well served by the use of “excellence” to determine the distribution of resources and incentives to the world’s researchers, teachers and research institutions.


> a focus on “excellence” impedes rather than promotes scientific and scholarly activity: it at the same time discourages both the intellectual risk-taking required to make the most significant advances in paradigm-shifting research and the careful “Normal Science” (Kuhn [1962] 2012)

Seems critical but does rely on a Kuhnian perspective on science. Let's see.


>The article itself falls into three parts. In the first section, we discuss “excellence” as a rhetoric. Drawing on work by Michèle Lamont and others, we argue that “excellence” is less a discoverable quality than a linguistic interchange mechanism by which researchers compare heterogeneous sets of disciplinary practices. In the second section, we dig more deeply into the question of “excellence” as an assessment tool: we show how it distorts research practice while failing to provide a reliable means of distinguishing among competing projects, institutions, or people. In the final section, we consider what it might take to change our thinking on “excellence” and the scarcity it presupposes. We consider alternative narratives for approaching the assessment of research activity, practitioners, and institutions and discuss ways of changing the “scarcity-thinking” that has led us to our current use of this fungible and unreliable term. We propose that a narrative built on “soundness” and “capacity” offers us the opportunity to focus on practice of productive research and on the crucial role that social communication and criticism plays.

Three sections of this paper:
1- EXCELLENCE AS RHETORIC
2- EXCELLENCE FAILS AS A FRAMEWORK FOR ASSESSMENT
3- ALTERNATIVES


>This tallies with the work of others who have considered reforms to the review process in recent years. Kathleen Fitzpatrick, for instance, has also situated the crux of evaluation in the evaluator, not the evaluated. For, as Fitzpatrick notes, “in using a human filtering system, the most important thing to have information about is less the data that is being filtered, than the human filter itself: who is making the decisions, and why. Thus, in a peer-to-peer review system, the critical activity is not the review of the texts being published, but the review of the reviewers.” 

Critical scholars of 'excellence', 'peer review', 'metrics' tend to reverse the pressure from the evaluated to the evaluator.


>This suggests that “excellence” resides between different communities and is ill-structured/defined in each context. Local groups and disciplines may have their own more specific (though sometimes conventional rather than explicit) measures of “excellence”: Biologists may treat some aspects of performance as “excellent” (for example, number of publications, author position, citations counts), while failing to recognize aspects considered equally or more “excellent” by English professors (large word counts, single authorship, publication or review in popular literary magazines and journals) (O’Donnell, 2015)

This is the main conclusion of the first section: excellence exists but is heavily context specific (here refers to context as discipline).

>Could “excellence” be, to speak bluntly, a linguistic signifier without any agreed upon referent whose value lies in an ability to capture cross-disciplinary value judgements and demonstrate the political desirability of public investment in research and research institutions?

This is the possibility even when accepting the locality of excellence: if we accept that excellence refers to local values, understanding the values of each discipline can serve as a a means to organise and distribute resources.


>Although, as its ubiquity suggests, “excellence” is used across disciplines to assert value judgements about otherwise incomparable scientific and scholarly endeavours, the concept itself mostly fails to capture the disciplinary qualities it claims to define.

This is a most severe claim in addition to the locality claim: even within specific contexts, excellence fails to perform as intended (i.e., representing the sought-after values of each discipline)


>As we have argued, the claim that a research project, institution, or practitioner is “excellent” is little more than an assertion that that project, institution, or practitioner can be said to succeed better on its own terms than some other project, institution, or practitioner can be said to succeed on some other, usually largely incomparable, set of terms.

'Excellence' is an post-hoc tag, added to define who (institution, project, person) won in a competition in which the rules of the game had not previously been  even made explicit.


>More worryingly, there is also considerable evidence of false positives in the review process—that is to say submissions that are judged to meet the standards of “excellence” required by one funding agency, journal, or institution, but do worse when measured against other or subsequent metrics. In a somewhat controversial work, Peters and Ceci submitted papers in slightly disguised form to journals that had previously accepted them for publication (Peters and Ceci, 1982; see Weller, 2001 for a critique). Only 8% overall of these resubmissions were explicitly detected by the editors or reviewers to which they were assigned. Of the resubmissions that were not explicitly detected, approximately 90% were ultimately rejected for methodological and/or other reasons by the same journals that had previously published them; they were rejected, in other words, for being insufficiently “excellent” by journals that had decided they were “excellent” enough to enter the literature previously.


>the rhetorical quality of “excellence” encourages researchers to submit fraudulent, erroneous, and irreproducible papers, at the same time as it works to prevent the publication of reproduction studies that can identify such work.

Misconduct is the *cause* of excellence


>The drive for “excellence” in the eyes of assessors is shown even more starkly in work by Chubb and Watermeyer (2016). In structured interviews, academics in Australia and the United Kingdom admitted to outright lies in the claims of broader impacts made in research proposals. As the authors note: “Having to sensationalize and embellish impact claims was seen to have become a normalized and necessary, if regretful, aspect of academic culture and arguably par for the course in applying for competitive research funds” (6). Quoting an interviewee, they continue, “If you can find me a single academic who hasn’t had to bullshit or bluff or lie or embellish to get grants, then I will find you an academic who is in trouble with his [sic] Head of Department” (6; “[sic]” as in Chubb and Watermeyer). Here we see how a competitive requirement, perceived or real, for “excellence”, in combination with a lack of belief in the ability of assessors to detect false claims, leads to a conception of “excellence” as pure performance: a concept defined by what you can get away with claiming in order to suggest (rather than actually accomplish) “excellence”.

Excellence drives people to make exageratted claims


>The motivation for these frauds, then as now, involves prestige and competition for resources.

More evidence that the source of fraud is competition for scare resources.


>In other words, the works that—and the people who—are considered “excellent” will always be evaluated, like the canon that shapes the culture that transmits it, on a conservative basis: past performance by preferred groups helps establish the norms by which future performances of “excellence” are evaluated. Whether it is viewed as a question of power and justice or simply as an issue of lost opportunities for diversity in the cultural coproduction of knowledge, an emphasis on the performance of “excellence” as the criterion for the distribution of resources and opportunity will always be backwards looking, the product of an evaluative process by institutions and individuals that is established by those who came before and resists disruptive innovation in terms of people as much as ideas or process.

Competition is benefitial for the *previous* winners, evaluation in terms of excellence drives resources via conservative means.


>Alternative narratives: working for change If, as we have argued, “excellence” in all its many forms and meanings is both unreliable as a measure of actual quality, and pernicious in the way it promotes poor behaviour and discourages good, what then are the alternatives? Given the political realities that have promoted the use of this rhetoric in defence of science and scholarship, are there other, less damaging ways in which we can evaluate and promote the value of research and its communication?

I find it great that there is a whole section dedicated to this. Very necessary.


>Gordon and Poulin argued that, for science funding in Canada through the National Science and Engineering Research Council (NSERC, the main STEM funding agency), it would have cost less at a whole system level simply to distribute the average award to all eligible applicants than to incur the costs associated with preparing, reviewing and selecting proposals (2009; although see Roorda, 2009 for a critique of their calculation). A rough calculation of the system costs of preparing failed grant applications would suggest that they are in the same order of magnitude as research grant funding itself (Herbert et al., 2013).

Alternative: fair distribution


>What this suggests is that “excellence” is not the only policy choice concerning the resourcing of research, nor even, necessarily, the only politically compelling one: from concentrating resources on the most deserving, allegedly “excellent”, institutions and researchers, to distributing them amongst all those that meet some minimum criteria—or even some subset, by lottery (Health Research Council of New Zealand, 2016; Fang et al., 2016)

Alternative: lottery


>In the context of scarce resources and a desire to maximize outcomes, indeed, there is even an argument for focussing most attention on the worst institutions; those that might most benefit from resources to improve (Bishop, 2013), have the greatest scope for improvement, and would go the longest way to ensuring an increase in basic capacity.

Alternative: funding those institutions that need it, and funding not based on output but based on practice. What about a funding mechanism that funds institutions that to well in terms of valuable practices, but rewarding more those that have the least income (evaluating best practices and correcting for wealth)


>But the evaluation of “soundness” is based in the practice of scholarship, whereas “excellence” is a characteristic of its objects (outputs and actors). In this sense “soundness” aligns well with approaches that locate the value of scholarship and evaluation in the nature of its processes (that is, “proper practice”) and its social conduct.

Soundness puts the emphasis more on the process of science, governemed by discipplinary standards, rather than the output (which is what excellence hints to)



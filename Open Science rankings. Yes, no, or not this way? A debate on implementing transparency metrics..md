# Open Science rankings: yes, no, or not this way? A debate on implementing transparency metrics.



## Speakers
What alternatives can we find (for academia)?

- [Etienne P LeBel](https://etiennelebel.com/), independent scientist, meta-scientist, psychologist. 
- [Chris Lorenz](https://scholar.google.be/citations?user=o7tCChsAAAAJ&hl=en), professor of historical theory, Institute for Social Movements (ISB), Ruhr-University Bochum. Author of 'If You're So Smart, Why Are You under Surveillance? Universities, Neoliberalism, and New Public Management' ([Lorenz 2012](Lorenz%202012.md))
- Representative from a low-income country




## Curate Science

[Website](https://curatescience.org/app/home#people)
Funded by a [Marie Curie Grant](https://cordis.europa.eu/project/rcn/215183/en)
Run by Etienne P LeBel


### What they bring
[Original tweet thread](https://twitter.com/curatescience/status/1371927234899017731)
> Science requires *minimum transparency*, conceptually & ethically, but because it's NOT yet enforced, most research is still not reported transparently. To help solve this problem, we're launching a new & ongoing initiative: Researcher TRANSPARENCY AUDITS! [https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html…](https://t.co/6T4oTTutDo?amp=1). 

> Analogous to tax audits, this involves checking the transparency (T) of researchers' recent publications, in relation to a specific T standard, whereby audited authors are given a chance to correct/add T information PRIOR to the public release of audit results. 

> Because we're in a transition period, & given we've carefully listened to the community's needs/concerns for 6+yrs, we've chosen the most GENEROUS & FEASIBLE min. T standard possible while still conforming to current ethical codes of conduct & foundational scientific principles. 

> Indeed, as can be seen, various EXEMPTIONS & GRANDFATHER provisions are offered to ensure researchers w/ valid reasons preventing them from meeting the standard are fairly accommodated (eg data that cannot be publicly posted due to ethical reasons). 

> Excited to reveal results from 1st round of audits via interactive T leaderboard. Majority of authors cooperated & impressively 90% ended up meeting the standard. Congrats! [@annaveer](https://twitter.com/annaveer) [@susafiedler](https://twitter.com/susafiedler) [@SimoneSchnall](https://twitter.com/SimoneSchnall) [@dingding\_peng](https://twitter.com/dingding_peng) [@BrianNosek](https://twitter.com/BrianNosek) [@LorneJCampbell](https://twitter.com/LorneJCampbell) [@DanTGilbert](https://twitter.com/DanTGilbert). 

> Of course small & non-representative sample & T standard is modest. But all of us meeting a modest T standard is still better than no one meeting any standard at all. Plus, T standard reflects a higher T level than vast majority of research (eg open data rates <15% across fields). 

> As a field we, believe it’s now the time to conduct T audits at SCALE for the benefit of all. Indeed, recent transparency positions echo this sentiment [@siminevazire](https://twitter.com/siminevazire) “T guarantees your research gets the credibility it deserves.” [@RogertheGS](https://twitter.com/RogertheGS) "trust in science must be earned"

> In this spirit let us know if you’re interested in being audited. This will expand the T leaderboard, amplifying the social contagion effect of initiative. As reward you can SIGNAL your transparency on your own website/uni page at researcher or article-level via our new T widgets. 

> Time is also ripe technologically. Several recently-developed (OS) transparency metadata tools can be used to scale up T audits. These tools automatically extract T metadata from articles, which a human auditor can then correct/add to in correspondence w/ audited authors.

> NEXT STEPS: To further demonstrate the feasibility of T audits, we will be conducting a 2nd round of (RANDOM) audits from a broader population frame in the fields of biology/biophysics and marine science with the help of interdisciplinary collaborators already on board.

> Thoughts, comments, and feedback welcome! Thanks to [@dominik\_lenda](https://twitter.com/dominik_lenda) [@hpashler](https://twitter.com/hpashler) [@mikemorrison](https://twitter.com/mikemorrison) who helped with or provided feedback on the design, development, and conduct of the audits.

> Weve used [carrots] to [improve] transparency for 25+yrs (eg CONSORT 1996). Its helped but today open data rates are abysmal (2-18% in psych/social sci/biomedicine). We FULLY support [carrots] but this shows we need MORE than just [carrots] or else trust, credibility & value of our profession will only further erode.

### Concerns

**Focus on individuals rather than groups**

> Roger Giner-Sorolla [@RogertheGS](https://twitter.com/RogertheGS) Please note that I have not been consulted on this initiative and I have reservations about its focus on researchers rather than journals, the metrics used, and the ranking aspect. [tweet](https://twitter.com/RogertheGS/status/1372122380689141760)

> Pedro Tiago Martins ([@ptsgmartins](https://twitter.com/ptsgmartins)) 
> Auditing publishers would probably contribute more to changing whatever needs to be changed; they have a responsibility to ensure transparency. Now that's a metric. Not all researchers have that responsibility (or even have a say... think ECRs or non senior authors in general). [tweet](https://twitter.com/ptsgmartins/status/1372901111993139200)


<br>

**Technical difficulty**
> Roger Giner-Sorolla [@RogertheGS](https://twitter.com/RogertheGS) 
> Of course we need to check and reflect on uptake of OS practices, but "audit" suggests a deeper and more difficult process of evaluating evidence for OS claims than is going on here (which is still fairly involved).[tweet](https://twitter.com/RogertheGS/status/1372131998807035905) or [tweet](https://twitter.com/RogertheGS/status/1372122380689141760)


<br>

**Unfair**

> [@jose_biurrun](https://twitter.com/jose_biurrun)
> I like the initiative, but mixing open access with transparency is a mistake IMO. You might want to think otherwise, but it's turning into a pay-for-publish model, that leaves underfunded researchers (f. ex. in developing countries) out of the system [tweet](https://twitter.com/jose_biurrun/status/1372273066068295689)
>> [@curatescience](https://twitter.com/curatescience)
>> as mentioned, "copyedited GREEN OA" meets requirements and is free, so meeting OA requirement is free for everyone. rationale for incl. OA in transparency, is that researchers (mostly legacy researchers) can use paywalls to shield their papers from more thorough scrutiny [tweet](https://twitter.com/curatescience/status/1372560739651756040) 
>>> [@jose_biurrun](https://twitter.com/jose_biurrun)
>>> Do you really think so? I mean, labeling a publication in a paywalled journal as "shielded" is quite a statement. And people who does "thorough scrutiny" is probably not stopped by a mere paywall. [tweet](https://twitter.com/jose_biurrun/status/1372588256789987330)
>>>> [@curatescience](https://twitter.com/curatescience)
>>>> well the vast majority of scientists and citizen scientists do NOT have access to paywallled papers. and sci-hub doesn't yet provide full coverage (& not everyone knows about sci-hub, particularly citizen scientists) [tweet](https://twitter.com/curatescience/status/1372590561555836935)
>>>>> [@jose_biurrun](https://twitter.com/jose_biurrun)
>>>>> Fair enough, and I'm actually pointing in the same direction, in which we may just not have resources to comply with these transparency standards [tweet](https://twitter.com/jose_biurrun/status/1372591909261209608)

> Alyssa Thomas ([@Dr_AlyssaThomas](https://twitter.com/Dr_AlyssaThomas))
> Not all researchers can provide their data, and for valid reasons. Especially for social scientists. [tweet](https://twitter.com/Dr_AlyssaThomas/status/1373655329666342920)

<br> 


**Bad marketting**
> Jay Patel, Maester ([@metasdl](https://twitter.com/metasdl)) I like the spirit of improvement, but is there a better term than "audit?" It may seem unwelcoming to many. Never had an audit that went well in my own life... "Transparency check" seems more neutral. [tweet](https://twitter.com/metasdl/status/1371991262182641665)

<br>



**problem of legitimacy**
https://twitter.com/oliviodare/status/1372267294001229825

> Ted Pedersen ([@SeeTedTalk](https://twitter.com/SeeTedTalk)). 
> transparency might suggest disclosing who is operating this account ... eg listing handles in account or signing tweets in some way ...?

> Swapnil Hiremath ([@hswapnil](https://twitter.com/hswapnil))
> why are you doing this? who asked you?

>> Bill Cooke ([@BillCookester](https://twitter.com/BillCookester)).
>> So there were two questions. You only answered one, and isn't even acknowledge the more important (who asked you to to do this). That is unethical of itself, and suggest s you are self selected and unaccountable power-heads. [tweets](https://twitter.com/BillCookester/status/1373060331572973571)

<br>

**the complicated relationship between transparency and impact**
> Egon Willighⓐgen([@egonwillighagen](https://twitter.com/egonwillighagen))
> what has the H-index to do with transparency? [tweet](https://twitter.com/egonwillighagen/status/1372466054245548033)
>> [@curatescience](https://twitter.com/curatescience)
>> The h-index is included as a further demonstration of its limitations. For eg, having a large h-index *WITHOUT* exhibiting a transparent track record could be seen as a liability (one's research is being cited, but not enough transparency to properly scrutinize/vet the research) [tweet](https://twitter.com/curatescience/status/1372626394249900046)
>> [@curatescience](https://twitter.com/curatescience)
>> but too many ppl are confused about this, so we will be removing it for now [18 mar.](https://twitter.com/curatescience/status/1372626506791391234)
>>> Katherine Bryant PhD ([@EvoNeuro](https://twitter.com/EvoNeuro))
>>> I don't think they're confused [tweet](https://twitter.com/EvoNeuro/status/1372839660246945793)

<br> 

**the problem of reducing employment to numerical evidence**

> Pedro Tiago Martins ([@ptsgmartins](https://twitter.com/ptsgmartins))
> Also, if we go this route of publicly quantifying everything about (perceived) individual performance, will selection committees actually have any agency at all? Might as well plug names into a computer, no application required. Some things are relative and have context. [tweets](https://twitter.com/ptsgmartins/status/1372901974413340672)


<br> 

**bad for science itself**

> Peter Edelsbrunner ([@peter1328](https://twitter.com/peter1328))
> Science is not (always) a competition, competition is seldom beneficial, enforcement through competition is seldom beneficial in science, and this is a step in the wrong direction.
 

> Warren Cook ([@warren_cook42](https://twitter.com/warren_cook42) The main function of this initiative, in practice, seems like it will be to divide the open science community. I sincerely hope this metric is never used for anything substantive or serious. I'd be more constructive, but I can't see a good, ethical, or useful way of doing this. [tweet](https://twitter.com/warren_cook42/status/1372603703606267905)




### my own additional questions
**how are standards calculated**

**how did they "listen to the community's needs/concerns?" **
<br> 



## Speaker questions

- What is the problem of today's academia?
	- Where is the root of the problem?


- What are the solutions?
	- What alternatives can we find (for academia)?

- What is the relation between free market and managerial practices in academia? Where can we identify each one?

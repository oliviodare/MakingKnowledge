# Bibliometrics: The Leiden Manifesto for research metrics
#biodiv/readingnote #üö´nonthesis 


## Metadata

|   Variable     |  |
|:--------------|:-----------|
| **Author**			| Hicks, Wouters, Waltman, de Rijcke, Rafols     | 
| **Year**				| 	2015		 | 
| **Title**				| 	'Bibliometrics: The Leiden Manifesto for research metrics'		 | 
| **Kind**				| Manifesto| 
| **Journal/Publisher**				| 	*Nature*		 | 
| **Date read**				| 	May 2021	 | 
| **Weblink**				| https://doi.org/10.1038/520429a			 | 

**Take-home message**: The Leiden manifesto: 10 rules known by bibliometricians about how to avoid misuse of metrics and to allow professional expertise and judgment lead evaluation rather than data.

---

## Reading Notes

> "The problem is that evaluation is now led by the data rather than by judgement.‚Äù (Hicks et al., 2015, p. 429)

This is the main problem of metrics at the moment: losing professional evaluation at the hands of data.

> "Metrics have proliferated: usually well intentioned, not always well informed, often ill applied.‚Äù (Hicks et al., 2015, p. 429)


> "We therefore present the Leiden Manifesto, named after the conference at which it crystallized (see http://sti2014.cwts.nl). Its ten principles are not news to scientometricians, although none of us would be able to recite them in their entirety because codification has been lacking until now. Luminaries in the field, such as Eugene Garfield (founder of the ISI), are on record stating some of these principles. But they are not in the room when evaluators report back to university administrators who are not expert in the relevant methodology. Scientists searching for literature with which to contest an evaluation find the material scattered in what are, to them, obscure journals to which they lack access.‚Äù (Hicks et al., 2015, p. 430)

The aim of the Leiden manifesto was to distill in very clear terms what bibliometricians know are 'best practices' so as to be able to use data and metrics as a tool for expert judgment, not a substitute. Interestingly, it seems to situate the misuse of metrics at the 'evaluator' level: those are the ones that should also be hold accountable.

Two aspects come to mind:

- who are these 'evaluators'? Are they university administrators with no or little background in research? Are researchers themselves in the committees of funding agencies?

- how can researchers hold evaluators into account? How can we make accountable those who evaluate our work to do it in a fair, evidence-based, manner?

> "We offer this distillation of best practice in metrics-based research assessment so that researchers can hold evaluators to account, and evaluators can hold their indicators to account.‚Äù (Hicks et al., 2015, p. 430)


> "assessors must not be tempted to cede decision-making to the numbers. Indicators must not substitute for informed judgement.‚Äù (Hicks et al., 2015, p. 430)


> "Measure performance against the research missions of the institution, group or researcher. Programme goals should be stated at the start, and the indicators used to evaluate performance should relate clearly to those goals. The choice of indicators, and the ways in which they are used, should take into account the wider socio-economic and cultural contexts.‚Äù (Hicks et al., 2015, p. 430)

This seems to imply that before counting, we have to understand the goals that have been set --> in our case at hand, is transparency a goal to be shared by all in science? Or is it a tool to achieve those goals?


> "Protect excellence in locally relevant research.‚Äù (Hicks et al., 2015, p. 430)

I am unclear on the relation between excellence and metrics. How can excellence be thought of in a local level? As well, it seems that a focus on excellence may also be detrimental and contribute to the Matthew effect.


> "Recent commercial entrants should be held to the same standards; no one should accept a black-box evaluation machine.‚Äù (Hicks et al., 2015, p. 430)

Here we see the other edge of transparency: not of the research itself, but of the tools of evaluating research.


> "Account for variation by field in publication and citation practices.‚Äù (Hicks et al., 2015, p. 430)

This criteria is certainly not met by CS.
> "The meaning of citation counts, for example, has long been debated. Thus, best practice uses multiple indicators to provide a more robust and pluralistic picture. If uncertainty and error can be quantified, for instance using error bars, this information should accompany published indicator values.‚Äù (Hicks et al., 2015, p. 431)

This is also another issue to confront CS: how is uncertainty managed in the transparency metrics?



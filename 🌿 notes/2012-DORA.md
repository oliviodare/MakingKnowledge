# San Francisco Declaration on Research Assessment
#biodiv/readingnote #ðŸš«nonthesis 

## Metadata

|   Variable     |  | 
|:--------------|:-----------|
| **Author**			| Declaration on Research Assessment (DORA)     | 
| **Year**				| 		2012	 | 
| **Title**				| 	'San Francisco Declaration on Research Assessment'		 | 
| **Kind**				| Manifesto	 | 
| **Date read**				| 	May 2021	 | 
| **Weblink**				| 	https://sfdora.org/read/		 | 

**Take-home message**: [[the problem that DORA tackles is an problem of employment]].



---

## Highlights and notes



 > There is a pressing need to improve the ways in which the output of scientific research is evaluated by funding agencies, academic institutions, and other parties.

This way of framing DORA as 'pressing' is not uncommon and actually very shared across the OS community, and in the CS's project as well. The question remains, however: why is it so pressing? What is evaluation such an important aspect in research?
 
 > The outputs from scientific research are many and varied, including: research articles reporting new knowledge, data, reagents, and software; intellectual property; and highly trained young scientists. Funding agencies, institutions that employ scientists, and scientists themselves, all have a desire, and need, to assess the quality and impact of scientific outputs. It is thus imperative that scientific output is measured accurately and evaluated wisely.
 
Here a more concrete framing is proposed: employment. There is a sense of urgency due to the fact that individual employment in research depends on outputs, and there is a need that the assesment for employment is done fairly. However, this is rarely explictly stated. 
 
 > Outputs other than research articles will grow in importance in assessing research effectiveness in the future, but the peer-reviewed research paper will remain a central research output that informs research assessment. Our recommendations therefore focus primarily on practices relating to research articles published in peer-reviewed journals but can and should be extended by recognizing additional products, such as datasets, as important research outputs. These recommendations are aimed at funding agencies, academic institutions, journals, organizations that supply metrics, and individual researchers.

Here the DORA authors are concerned with the main research output of scientific assessment: the article. However, they do open the door and invite others to use their recommendations for other sets of metrics. 
 
 > **General Recommendation**
 >  1. Do not use journal-based metrics, such as Journal Impact Factors, as a surrogate measure of the quality of individual research articles, to assess an individual scientistâ€™s contributions, or in hiring, promotion, or funding decisions.

This is their main recommendation. Again, employment is the main issue that DORA tackles. 

>  For the purposes of research assessment, consider the value and impact of all research outputs (including datasets and software) in addition to research publications, and consider a broad range of impact measures including qualitative indicators of research impact, such as influence on policy and practice.
 
 This is repeated twice, and can be read as support for and defence of a pluralistic view of scientific output. 

>  **For organizations that supply metrics**

These are guidelines which directly address the CS project
 >  11- Be open and transparent by providing data and methods used to calculate all metrics.

Seemingly clear (at least on the methods and data used), but not very clear on who runs CS.

 > 12- Provide the data under a licence that allows unrestricted reuse, and provide computational access to data, where possible.
 
 ?
 
 > 13- Be clear that inappropriate manipulation of metrics will not be tolerated; be explicit about what constitutes inappropriate manipulation and what measures will be taken to combat this.

Not to my knowledge

>  14- Account for the variation in article types (e.g., reviews versus research articles), and in different subject areas when metrics are used, aggregated, or compared.

Certainly not: CS is explicitly designed as non-pluralistic and centralising ('minimum transparency')

# The data-hypothesis relationship

## Metadata

**Authors**: Felin, Koenderink, Krueger, Noble and Ellis
**Year**: 2021
**Title**: The data-hypothesis relationship
**Kind**: Response 
**Journal/Publisher**: *Genome Biology*
**Date read**: 18 Aug 2021
**Tags**: #scientificpaper #scientificmethod
**Zotero link**: [](zotero link)
**Weblink**: https://doi.org/10.1186/s13059-021-02276-4
**A response to**: [2020-Yanai and Lercher](2020-Yanai%20and%20Lercher.md)
**Response**: [[2021-Yanai and Lercher]]
**Take-home message**: 
There are various points that critique [2020-Yanai and Lercher](2020-Yanai%20and%20Lercher.md)'s editorial. The two main points is 1) that Yanai and Lercher are reductionist and naïve in that they equate data to information, and 2) that the hypothesis come in different forms and shapes, and together with theories, conjectures, and suppositions, they mediate our understanding of data and frame research projects. 

---

## Reading notes

> Hypotheses are needed. Thus, there is a mismatch between the experiment and what Y&L are claiming, on a number of levels.

> The problem is that—whether in science or in everyday life—an indefinite number of things remain undetected when we interact with data or visual scenes. It is not obvious what an apple falling means, without the right question, hypothesis, or theory. Visual scenes and data teem with possibilities, uses and meanings.

Problem 1: hypothesis and theories give meaning to observations.

> Reductionist forms of science assume that cues and data (somehow) jump out and tell us why they are relevant and important, based on the characteristics of the data itself (the physical properties of the world). In vision science, this assumption is based on research in psychophysics (and inverse optics and ideal observer theory) that focuses on salience as a function of cue or stimulus characteristics. From this perspective, cues and stimuli become data, information, and evidence due to their inherent nature [5].

Problem 2: reductionism, naïve view that data speaks for itself.

> The key point here is that the “transformation” of raw cues or data to information and evidence is not a straightforward process. It requires some form of hypothesis. Cues and data do not automatically tell us what they mean, whether or why they are relevant, or for which hypothesis they might provide evidence. Size is relevant in some situations, but not in others. Cues and data only become information and evidence in response to the questions and queries that we are asking.

This is a very nice point about the function of hypothesis: they mediate the 'transformation' from data to evidence by introducing a bias, a frame.

> Ofcourse,theideaofengagingina fishing expedition—as Y&L recognize—has highly negative connotations, suggesting haphazard, unscientific, and perhaps even unethical practices.

It is unclear why exactly the metaphor of a 'fishing expedition' may be unscientific or unethical. However, what they do claim is that even exploration are theory-, hypothesis-, and value-laden.

> Any look at data—however preliminary it might be—necessarily represents some form of proto-hypothesis: a latent expectation, question, or even guess about what might be lurking, about what might potentially be interesting or relevant and how it might be caught.

Thus hypothesis is not an estable category: there are degrees of certainty, as there are degrees of finished hypothesis.

> Now, it might seem like we are stretching the definition of a hypothesis by including expectations, conjectures, and even the statistical and computational tools that are used to generate insights. But we think it is important to recognize that any tool—whether cognitive, computational, or statistical—functions like a net, as it already embodies implicit hypotheses about what matters and what does not.

Various definitions of hypothesis that have several levels of *formality*.

> Science is about making decisions about what subset of all this “stuff” should be focused on and included in the analysis.

Science needs meaning.

> And the data analysis itself is theory-based [13]: it depends on templates of waves expected from the gravitational coalescence of black holes or neutron stars.

Problem 3. This is important too: while collecting data, we already have theory in place. Another example would be fMRI. With a fMRI dataset, one one may say that it doesn't make any theories about the brain. However, the instrumentation and data manipulation analysis that are necessary to obtain the 'raw data' do depend on physical properties of magnetism. Setting up the machine needs of a theory of electromagnetism. 'Cleaning' data needs of an understanding of how magnetic waves typically interact with life bodies and human flesh in particular.

> Our concern is that starting at the bottom—as suggested by Y&L’s notion of hypothesis-free exploration of data—will inadvertently lead to an overly descriptive science: what Ernest Rutherford called “stamp collecting.”

What do we mean here by 'descriptive', as opposed to 'normative'?